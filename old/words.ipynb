{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31368156-c152-44c2-b786-f9849da71c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "information technology capability:  10%|▉         | 55001/566201 [10:20<1:36:06, 88.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有符合條件的數據已保存到 information technology capability.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "organizational resilience:  28%|██▊       | 19501/68840 [04:30<11:23, 72.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有符合條件的數據已保存到 organizational resilience.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定 CrossRef API 的基本參數\n",
    "CROSSREF_BASE_URL = \"https://api.crossref.org/works\"\n",
    "\n",
    "# 設定 OpenCitations API 的基本參數\n",
    "OPENCITATIONS_INDEX_URL = \"https://opencitations.net/index/api/v2/citation-count/doi:\"\n",
    "\n",
    "# 設定查詢條件\n",
    "search_querys = [\"'information technology capability'\", \"'organizational resilience'\"]\n",
    "targets = [50, 10]  # 最低引用次數/年\n",
    "\n",
    "for n, search_query in enumerate(search_querys):\n",
    "    result = []\n",
    "\n",
    "    # 查詢參數\n",
    "    rows = 500  # 每次請求的結果數量（CrossRef 建議使用較小的數量以避免過載）\n",
    "    offset = 0  # 每次偏移量\n",
    "    cursor = \"*\"  # 起始游標\n",
    "    target = targets[n]  # 最低引用次數/年\n",
    "    flag = True\n",
    "\n",
    "    # 初始化 CSV 文件\n",
    "    csv_file = search_query.replace(\"'\", \"\").replace(\" \", \"_\")+\".csv\"\n",
    "    result.append([\"Title\", \"Abstract\", \"Citations\", \"Year\", \"Link\"])\n",
    "\n",
    "    # 構建 CrossRef 請求參數\n",
    "    params = {\n",
    "        \"query.bibliographic\": search_query,\n",
    "        \"rows\": rows,\n",
    "        \"select\": \"title,abstract,published-print,published-online,created,URL,DOI,is-referenced-by-count\",\n",
    "        \"filter\": \"from-pub-date:2020-01-01,has-abstract:true\",\n",
    "        \"sort\": \"is-referenced-by-count\",\n",
    "        \"order\": \"desc\",\n",
    "        \"cursor\": cursor\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 發送 GET 請求到 CrossRef API\n",
    "        response = requests.get(\n",
    "            CROSSREF_BASE_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()  # 如果響應狀態碼不是 200，則引發異常\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching CrossRef results: {e}\")\n",
    "        flag = False\n",
    "\n",
    "    # 解析 JSON 響應\n",
    "    data = response.json()\n",
    "\n",
    "    # 獲取總結果數量（僅在第一次請求時獲取）\n",
    "    total_results = data[\"message\"][\"total-results\"]\n",
    "    if total_results == 0:\n",
    "        print(f\"無查詢結果。\")\n",
    "        flag = False\n",
    "\n",
    "    # 獲取當前頁的項目\n",
    "    items = data[\"message\"][\"items\"]\n",
    "    if not items:\n",
    "        print(\"無查詢項目。\")\n",
    "        flag = False\n",
    "\n",
    "    # 獲取下一頁的游標\n",
    "    cursor = data[\"message\"][\"next-cursor\"]\n",
    "\n",
    "    loop = tqdm(range(total_results))\n",
    "    loop.set_description(search_query.replace(\"'\", \"\"))\n",
    "    for i in loop:\n",
    "        if i >= rows and i % rows == 0:\n",
    "            # 構建 CrossRef 請求參數\n",
    "            params = {\n",
    "                \"query.bibliographic\": search_query,\n",
    "                \"rows\": rows,\n",
    "                \"select\": \"title,abstract,published-print,published-online,created,URL,DOI,is-referenced-by-count\",\n",
    "                \"filter\": \"from-pub-date:2020-01-01,has-abstract:true\",\n",
    "                \"sort\": \"is-referenced-by-count\",\n",
    "                \"order\": \"desc\",\n",
    "                \"cursor\": cursor\n",
    "            }\n",
    "            try:\n",
    "                # 發送 GET 請求到 CrossRef API\n",
    "                response = requests.get(\n",
    "                    CROSSREF_BASE_URL, params=params, timeout=10)\n",
    "                response.raise_for_status()  # 如果響應狀態碼不是 200，則引發異常\n",
    "\n",
    "                # 解析 JSON 響應\n",
    "                data = response.json()\n",
    "\n",
    "                # 獲取當前頁的項目\n",
    "                items = data[\"message\"][\"items\"]\n",
    "                if not items:\n",
    "                    print(\"無查詢項目。\")\n",
    "                    continue\n",
    "\n",
    "                # 獲取下一頁的游標\n",
    "                cursor = data[\"message\"][\"next-cursor\"]\n",
    "                offset += rows\n",
    "\n",
    "                # 為避免觸發速率限制，適當添加延遲\n",
    "                time.sleep(0.3)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching CrossRef results: {e}\")\n",
    "                continue\n",
    "\n",
    "        item = items[i-offset]\n",
    "\n",
    "        # 提取 DOI\n",
    "        doi = item.get(\"DOI\", None)\n",
    "\n",
    "        # 提取被引用次數\n",
    "        cite = item.get(\"is-referenced-by-count\", 0)\n",
    "        if cite <= target // 10:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "        # 提取發布年份\n",
    "        if \"published-print\" in item and \"date-parts\" in item[\"published-print\"]:\n",
    "            published_year = item[\"published-print\"][\"date-parts\"][0][0]\n",
    "        elif \"published-online\" in item and \"date-parts\" in item[\"published-online\"]:\n",
    "            published_year = item[\"published-online\"][\"date-parts\"][0][0]\n",
    "        elif \"created\" in item and \"date-parts\" in item[\"created\"]:\n",
    "            published_year = item[\"created\"][\"date-parts\"][0][0]\n",
    "        else:\n",
    "            published_year = 0\n",
    "\n",
    "        # 檢查缺值\n",
    "        if doi and published_year >= 2020 and cite >= target:\n",
    "            cite_year = cite/(2025-published_year)\n",
    "            if cite_year >= 100:\n",
    "                # 提取標題\n",
    "                title = item.get(\"title\", \"No title available\")[\n",
    "                    0].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "                # 提取摘要\n",
    "                abstract = item.get(\n",
    "                    \"abstract\", \"No abstract available\")\n",
    "                abstract = re.sub(\n",
    "                    r\"\\s{2,}\", \" \", abstract, flags=re.I | re.M)\n",
    "                abstract = re.sub(r\"<[^>]*>\", \"\", abstract,\n",
    "                                  flags=re.I | re.M).strip()\n",
    "\n",
    "                # 提取連結\n",
    "                link = item.get(\"URL\", \"No link available\")\n",
    "\n",
    "                # 使用 OpenCitations API 獲取引用次數\n",
    "                citations = 0\n",
    "                citations_year = 0\n",
    "\n",
    "                # OpenCitations 使用 DOI 來查詢引用次數/年\n",
    "                try:\n",
    "                    header = {\n",
    "                        \"authorization\": \"6b17ce4d-1339-4e73-aa08-1ca3be1ebda6\"}\n",
    "                    # 查詢被引用次數\n",
    "                    opencitations_url = OPENCITATIONS_INDEX_URL+doi\n",
    "                    opencitations_response = requests.get(\n",
    "                        opencitations_url, headers=header)\n",
    "                    if opencitations_response.status_code == 200:\n",
    "                        citations_data = opencitations_response.json()\n",
    "                        citations = int(citations_data[0][\"count\"])\n",
    "                    else:\n",
    "                        print(opencitations_response.content)\n",
    "                    if published_year > 0 and citations > 0:\n",
    "                        citations_year = citations / \\\n",
    "                            (2025-published_year)\n",
    "\n",
    "                    # 為避免觸發速率限制，適當添加延遲\n",
    "                    time.sleep(0.3)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error fetching OpenCitations for DOI {doi}: {e}\")\n",
    "\n",
    "                # 應用過濾條件：引用次數大於 target\n",
    "                if citations_year >= target:\n",
    "                    result.append(\n",
    "                        [title, abstract, citations, published_year, link])\n",
    "\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for x in result:\n",
    "            # 寫入\n",
    "            writer.writerow(x)\n",
    "    print(f\"所有符合條件的數據已保存到 {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d7ad9",
   "metadata": {},
   "source": [
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12b1d9d-e38d-4f0a-9e0e-29cbb33e2aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          phrase  frequency\n",
      "7701    research         30\n",
      "5248    learning         28\n",
      "2281        data         27\n",
      "9032  technology         22\n",
      "9563        used         21\n",
      "7880      review         19\n",
      "6050         new         19\n",
      "1517     chatgpt         18\n",
      "9533         use         18\n",
      "6524    patients         17\n",
      "          phrase  frequency\n",
      "1891    pandemic         17\n",
      "2284  resilience         14\n",
      "844     distress         13\n",
      "2238    research         13\n",
      "2331      review         12\n",
      "610        covid         12\n",
      "987       energy         11\n",
      "471      concept         10\n",
      "2880         vsc          9\n",
      "946    emotional          8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "for search_query in search_querys:\n",
    "    csv_file = search_query.replace(\"'\", \"\").replace(\" \", \"_\")+\".csv\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 讀取清理過的文本\n",
    "    cleaned_text = \"\"\n",
    "    for x in df[\"Abstract\"]:\n",
    "        cleaned_text += x\n",
    "\n",
    "    # 自定義停用詞\n",
    "    custom_stop_words = {\"et\", \"al\", \"amp\", \"bob\", \"yoo\", \"form\",\n",
    "                        \"ing\", \"st\", \"es\", \"lower\", \"billion\", \"na\", \"dt\", \"dts\"}\n",
    "\n",
    "    # 將自定義停用詞與內建的英語停用詞結合\n",
    "    stop_words = list(text.ENGLISH_STOP_WORDS.union(custom_stop_words))  # 轉換為列表格式\n",
    "\n",
    "    # 使用 CountVectorizer 計算詞頻，支持生成詞句 (bi-grams, tri-grams)，並排除數字和單一字母\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        token_pattern=r\"\\b[a-zA-Z]{2,}\\b\",  # 僅保留兩個或更多字母的詞彙\n",
    "        ngram_range=(1, 3)  # 設置為 bi-grams 和 tri-grams\n",
    "    )\n",
    "    X = vectorizer.fit_transform([cleaned_text])\n",
    "\n",
    "    # 獲取所有詞句和其詞頻\n",
    "    phrases = vectorizer.get_feature_names_out()\n",
    "    frequencies = X.toarray().flatten()\n",
    "\n",
    "    # 創建詞頻數據框\n",
    "    df_tf = pd.DataFrame({\"phrase\": phrases, \"frequency\": frequencies})\n",
    "\n",
    "    # 按詞頻排序並顯示前 10 個最常見的詞句\n",
    "    df_tf = df_tf.sort_values(by=\"frequency\", ascending=False)\n",
    "    print(df_tf.head(10))\n",
    "\n",
    "    # 存儲為 CSV 文件\n",
    "    output_csv_path = csv_file[:-4]+\"_TF.csv\"\n",
    "    df_tf.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104ba11",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68f2795-b63f-453f-bf22-d274d00b2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          phrase  tfidf_score\n",
      "7701    research     0.167418\n",
      "5248    learning     0.156256\n",
      "2281        data     0.150676\n",
      "9032  technology     0.122773\n",
      "9563        used     0.117192\n",
      "7880      review     0.106031\n",
      "6050         new     0.106031\n",
      "1517     chatgpt     0.100451\n",
      "9533         use     0.100451\n",
      "6524    patients     0.094870\n",
      "          phrase  tfidf_score\n",
      "1891    pandemic     0.206596\n",
      "2284  resilience     0.170138\n",
      "844     distress     0.157985\n",
      "2238    research     0.157985\n",
      "2331      review     0.145833\n",
      "610        covid     0.145833\n",
      "987       energy     0.133680\n",
      "471      concept     0.121527\n",
      "2880         vsc     0.109375\n",
      "946    emotional     0.097222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "for search_query in search_querys:\n",
    "    csv_file = search_query.replace(\"'\", \"\").replace(\" \", \"_\")+\".csv\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # 讀取清理過的文本\n",
    "    cleaned_text = \"\"\n",
    "    for x in df[\"Abstract\"]:\n",
    "        cleaned_text += x\n",
    "\n",
    "    # 自定義停用詞\n",
    "    custom_stop_words = {\"et\", \"al\", \"amp\", \"bob\", \"yoo\", \"form\",\n",
    "                        \"ing\", \"st\", \"es\", \"lower\", \"billion\", \"na\", \"dt\", \"dts\"}\n",
    "\n",
    "    # 將自定義停用詞與內建的英語停用詞結合\n",
    "    stop_words = list(text.ENGLISH_STOP_WORDS.union(custom_stop_words))  # 轉換為列表格式\n",
    "\n",
    "    # 使用 TfidfVectorizer 計算 TF-IDF，支持生成詞句 (bi-grams, tri-grams)，並排除數字和單一字母\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        token_pattern=r\"\\b[a-zA-Z]{2,}\\b\",  # 僅保留兩個或更多字母的詞彙\n",
    "        ngram_range=(1, 3)  # 設置為 bi-grams 和 tri-grams\n",
    "    )\n",
    "    X = vectorizer.fit_transform([cleaned_text])\n",
    "\n",
    "    # 獲取所有詞句和其 TF-IDF 分數\n",
    "    phrases = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X.toarray().flatten()\n",
    "\n",
    "    # 創建 TF-IDF 數據框\n",
    "    df_tfidf = pd.DataFrame({\"phrase\": phrases, \"tfidf_score\": tfidf_scores})\n",
    "\n",
    "    # 按 TF-IDF 分數排序並顯示前 10 個最重要的詞句\n",
    "    df_tfidf = df_tfidf.sort_values(by=\"tfidf_score\", ascending=False)\n",
    "    print(df_tfidf.head(10))\n",
    "\n",
    "    # 存儲為 CSV 文件\n",
    "    output_csv_path = csv_file[:-4]+\"_TF-IDF.csv\"\n",
    "    df_tfidf.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52021dbb",
   "metadata": {},
   "source": [
    "KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f481cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/md0/home/ansa1019/anaconda3/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: biotechnology information, Score: 0.5919\n",
      "Keywords: biotechnology, Score: 0.5445\n",
      "Keywords: applied biotechnology, Score: 0.5087\n",
      "Keywords: biotechnology biomedical, Score: 0.4908\n",
      "Keywords: center biotechnology, Score: 0.488\n",
      "Keywords: bioinformatics, Score: 0.4812\n",
      "Keywords: science pedagogy, Score: 0.4776\n",
      "Keywords: based biophysical, Score: 0.4767\n",
      "Keywords: edtech, Score: 0.4664\n",
      "Keywords: science education, Score: 0.4647\n",
      "Keywords: edtech start, Score: 0.4536\n",
      "Keywords: pubmed science, Score: 0.4505\n",
      "Keywords: advances bioinformatics, Score: 0.4429\n",
      "Keywords: scientific community, Score: 0.4399\n",
      "Keywords: life science, Score: 0.4393\n",
      "Keywords: web science, Score: 0.4308\n",
      "Keywords: biofunctionality provide, Score: 0.4302\n",
      "Keywords: online learning, Score: 0.429\n",
      "Keywords: online information, Score: 0.4284\n",
      "Keywords: online teaching, Score: 0.4222\n",
      "Keywords: science journals, Score: 0.42\n",
      "Keywords: laboratory, Score: 0.4192\n",
      "Keywords: biophysical, Score: 0.4158\n",
      "Keywords: crop biophysical, Score: 0.4149\n",
      "Keywords: numerous technologic, Score: 0.4146\n",
      "Keywords: stem disciplines, Score: 0.4144\n",
      "Keywords: importance online, Score: 0.4134\n",
      "Keywords: www ncbi, Score: 0.4117\n",
      "Keywords: literature information, Score: 0.4114\n",
      "Keywords: literature discussed, Score: 0.4101\n",
      "Keywords: journals ncbi, Score: 0.4088\n",
      "Keywords: impact academia, Score: 0.4081\n",
      "Keywords: biomedical industries, Score: 0.406\n",
      "Keywords: biology, Score: 0.4028\n",
      "Keywords: scholarly, Score: 0.4021\n",
      "Keywords: biology including, Score: 0.4019\n",
      "Keywords: biofunctionality, Score: 0.4013\n",
      "Keywords: importance bibliographic, Score: 0.4011\n",
      "Keywords: learning article, Score: 0.4006\n",
      "Keywords: analyses literature, Score: 0.4005\n",
      "Keywords: databases pubmed, Score: 0.3973\n",
      "Keywords: academia scholarly, Score: 0.395\n",
      "Keywords: literature review, Score: 0.3949\n",
      "Keywords: biochemical, Score: 0.3944\n",
      "Keywords: information resources, Score: 0.3934\n",
      "Keywords: technologies clinical, Score: 0.3932\n",
      "Keywords: field education, Score: 0.3929\n",
      "Keywords: enabling technologies, Score: 0.3924\n",
      "Keywords: review resilience, Score: 0.5135\n",
      "Keywords: compartmentalized adaptation, Score: 0.5126\n",
      "Keywords: systemic shift, Score: 0.5123\n",
      "Keywords: adaptation systemic, Score: 0.5016\n",
      "Keywords: resilience context, Score: 0.4964\n",
      "Keywords: resilience organizational, Score: 0.4951\n",
      "Keywords: change mitigation, Score: 0.4935\n",
      "Keywords: concept resilience, Score: 0.4911\n",
      "Keywords: resilience link, Score: 0.4903\n",
      "Keywords: resilience sustainability, Score: 0.4894\n",
      "Keywords: resilience construct, Score: 0.4842\n",
      "Keywords: resilience age, Score: 0.4833\n",
      "Keywords: adaptation, Score: 0.4822\n",
      "Keywords: implications strategy, Score: 0.477\n",
      "Keywords: resilience central, Score: 0.4742\n",
      "Keywords: transformations resilience, Score: 0.4741\n",
      "Keywords: survive changing, Score: 0.4685\n",
      "Keywords: resilience protective, Score: 0.4672\n",
      "Keywords: resilience suggestions, Score: 0.4623\n",
      "Keywords: cognitive reserve, Score: 0.4563\n",
      "Keywords: review antecedents, Score: 0.4524\n",
      "Keywords: resilience viability, Score: 0.4512\n",
      "Keywords: related changes, Score: 0.4478\n",
      "Keywords: reserve cognitive, Score: 0.4445\n",
      "Keywords: criticize resilience, Score: 0.4444\n",
      "Keywords: important insight, Score: 0.4443\n",
      "Keywords: important approaches, Score: 0.444\n",
      "Keywords: organizational resilience, Score: 0.4429\n",
      "Keywords: reserve resilience, Score: 0.4418\n",
      "Keywords: emerging directions, Score: 0.4411\n",
      "Keywords: findings interpretations, Score: 0.4407\n",
      "Keywords: resilient sustainable, Score: 0.4403\n",
      "Keywords: changing environment, Score: 0.4361\n",
      "Keywords: need assess, Score: 0.435\n",
      "Keywords: pandemic discussion, Score: 0.4339\n",
      "Keywords: strategies associated, Score: 0.4334\n",
      "Keywords: new insights, Score: 0.4321\n",
      "Keywords: abstractrecently concept, Score: 0.4318\n",
      "Keywords: review existing, Score: 0.4316\n",
      "Keywords: digital transformation, Score: 0.43\n",
      "Keywords: phenomenon dt, Score: 0.43\n",
      "Keywords: reappraisal reframing, Score: 0.4295\n",
      "Keywords: examine roles, Score: 0.4291\n",
      "Keywords: continuous adaptation, Score: 0.4262\n",
      "Keywords: brain maintenance, Score: 0.4234\n",
      "Keywords: perspectives synthesizing, Score: 0.4229\n",
      "Keywords: auspices alzheimer, Score: 0.4225\n",
      "Keywords: reframing conclusionsin, Score: 0.4224\n",
      "Keywords: analysis need, Score: 0.4221\n",
      "Keywords: resilience fuzzy, Score: 0.422\n",
      "Keywords: unfolding pandemic, Score: 0.4216\n",
      "Keywords: emphasize resilience, Score: 0.421\n",
      "Keywords: interpretations existing, Score: 0.4208\n",
      "Keywords: considered review, Score: 0.4192\n",
      "Keywords: relations resilience, Score: 0.4187\n",
      "Keywords: reappraisal, Score: 0.4185\n",
      "Keywords: transformation dt, Score: 0.4185\n",
      "Keywords: diverse fragmented, Score: 0.4185\n",
      "Keywords: resilience, Score: 0.4171\n",
      "Keywords: resilience light, Score: 0.4167\n",
      "Keywords: impacts cities, Score: 0.4159\n",
      "Keywords: decarbonization concept, Score: 0.4159\n",
      "Keywords: existing thematic, Score: 0.4135\n",
      "Keywords: cognitive, Score: 0.4127\n",
      "Keywords: innovations relationship, Score: 0.4123\n",
      "Keywords: brain reserve, Score: 0.4107\n",
      "Keywords: clarifying, Score: 0.4106\n",
      "Keywords: interventions potentially, Score: 0.4088\n",
      "Keywords: transformation, Score: 0.4087\n",
      "Keywords: strategies measured, Score: 0.4081\n",
      "Keywords: review techniques, Score: 0.4072\n",
      "Keywords: city concept, Score: 0.4057\n",
      "Keywords: analyse existing, Score: 0.4057\n",
      "Keywords: practical implications, Score: 0.4055\n",
      "Keywords: interpretations, Score: 0.4047\n",
      "Keywords: term impacts, Score: 0.4046\n",
      "Keywords: analyses, Score: 0.4046\n",
      "Keywords: change increasing, Score: 0.4038\n",
      "Keywords: synthesizing knowledge, Score: 0.4038\n",
      "Keywords: resilience gained, Score: 0.4024\n",
      "Keywords: synthesizing empirical, Score: 0.4018\n",
      "Keywords: types interventions, Score: 0.4015\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "\n",
    "for search_query in search_querys:\n",
    "    csv_file = search_query.replace(\"'\", \"\").replace(\" \", \"_\")+\".csv\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # 讀取清理過的文本\n",
    "    cleaned_text = \"\"\n",
    "    for x in df[\"Abstract\"]:\n",
    "        cleaned_text += x+\"\\n\"\n",
    "\n",
    "    # 加載 KeyBERT 模型\n",
    "    model = KeyBERT('Paraphrase-mpnet-base-v2')\n",
    "\n",
    "    # 提取關鍵詞\n",
    "    keywords = model.extract_keywords(cleaned_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=100)\n",
    "\n",
    "    # 過濾掉包含數字的關鍵詞並篩選分數大於0.6的\n",
    "    nowords = r\"\\d|biochar|research\"\n",
    "    filtered_keywords = [(keyword, score) for keyword, score in keywords if not re.search(nowords, keyword) and score > 0.3]\n",
    "\n",
    "    # 輸出關鍵詞\n",
    "    for keyword, score in filtered_keywords:\n",
    "        print(f'Keywords: {keyword}, Score: {score}')\n",
    "\n",
    "    # 將過濾後的關鍵詞轉換為 DataFrame\n",
    "    df_keywords = pd.DataFrame(filtered_keywords, columns=['Keywords', 'Score'])\n",
    "\n",
    "    # 存儲為 CSV 文件\n",
    "    output_csv_path = csv_file[:-4]+\"_KeyBERT.csv\"\n",
    "    df_keywords.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
